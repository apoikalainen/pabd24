# Предиктивная аналитика больших данных

Учебный проект для демонстрации основных этапов жизненного цикла проекта предиктивной аналитики.  

## Installation 

Клонируйте репозиторий, создайте виртуальное окружение, активируйте и установите зависимости:  

```sh
git clone https://github.com/yourgit/pabd24
cd pabd24
python -m venv venv

source venv/bin/activate  # mac or linux
.\venv\Scripts\activate   # windows

pip install -r requirements.txt
```

## Usage

### 1. Сбор данных о ценах на недвижимость 

Как работает:
1) Создание экземпляра парсера:
Объект CianParser создается для конкретного города (в данном случае, Москва).
2) Основная функция main():
  * Определяется текущее время для создания уникального имени файла.
  * Указывается количество комнат в квартире (например, 3).
  * Определяется путь для сохранения данных в формате CSV.
  * Используя метод get_flats библиотеки cianparser, собираются данные о трехкомнатных квартирах на продажу.
  * Дополнительные параметры:
    - Тип сделки: продажа (deal_type="sale").
    - Количество комнат: 3.
    - Сохранение в CSV: автоматически сохраняет также в CSV.
    - Дополнительные настройки:
      - Начальная и конечная страницы для парсинга (1 и 2).
      - Тип объекта: вторичное жилье.
  * Полученные данные преобразуются в формат DataFrame с использованием библиотеки pandas.
  * Данные сохраняются в указанный CSV файл с указанной кодировкой (utf-8) и без индекса.

Чтобы собрать данные о ценах на недвижимость необходимо:
1) Открыть файл [parse_cian.py](../src/parse_cian.py) и поменять следующие параметры в функции main():
 ```
 #...
 n_rooms = ...
 #...
additional_settings={
            "start_page": ...,
            "end_page": ...,
            "object_type": "..."
        }
 ``` 
2) Запустить файл:
```sh
python src/parse_cian.py
```

### 2. Выгрузка данных в хранилище S3 

Как работает:  
1)  Задание параметров:
  * BUCKET_NAME: имя целевого S3 бакета.
  * YOUR_ID: идентификатор пользователя или проекта.
  * CSV_PATH: список путей к локальным CSV-файлам, которые будут загружены в S3.
2) Загрузка конфигурации из файла .env:
Значения для KEY и SECRET загружаются из .env файла для обеспечения безопасности.
3) Функция main():
  * Создание клиента S3: Используются учетные данные из .env файла и указанный endpoint_url для подключения к S3 совместимому хранилищу (в данном случае Яндекс Облако).
  * Загрузка файлов:
    - Каждый файл из списка args.input загружается в указанное хранилище S3.
    - Имя объекта в бакете формируется на основе локального пути с добавлением идентификатора пользователя.

Чтобы выгрузить данные в хранилище S3 нужно:
1) Скопировать файл \.env в корень проекта.
2) В файле [upload_to_s3.py](../src/upload_to_s3.py) задать параметры:
```
BUCKET_NAME = '...'
YOUR_ID = '...'
CSV_PATH =['data/raw/...',
            'data/raw/...',
            'data/raw/...']
```
3) Запустить файл 
```sh
python src/upload_to_s3.py
```
### 3. Загрузка данных из S3 на локальную машину  

Как работает:
1) Задание параметров: 
  * BUCKET_NAME: Имя бакета в S3, куда будут загружены файлы.
  * YOUR_ID: Идентификатор пользователя или проекта, используемый для создания уникальных имен файлов в бакете S3.
  * CSV_PATH: Список локальных файлов CSV для загрузки в S3 по умолчанию.
2) Загрузка конфигурации из файла .env:
Значения для KEY и SECRET загружаются из .env файла для обеспечения безопасности.
3) Основная функция main()
Функция main() выполняет выгрузку файлов в S3. Она принимает аргументы командной строки и для каждого файла из списка путей к файлам (args.input) осуществляет загрузку в указанный бакет S3.
  * remote_name: Имя файла в бакете S3 формируется путем добавления идентификатора пользователя к пути локального файла.
  * Метод client.upload_file загружает локальный файл по указанному пути в S3 бакет.

Чтобы выгрузить данные из хранилища S3 нужно:  
1) В файле [download_from_s3.py](../src/download_from_s3.py) задать параметры:
```
BUCKET_NAME = '...'
YOUR_ID = '...'
CSV_PATH = ['data/raw/...',
            'data/raw/...',
            'data/raw/...']
```
2) Запустить файл 
```sh
python src/download_from_s3.py
```
### 4. Предварительная обработка данных  

1) Задание параметров
  * IN_FILES: Список путей к исходным файлам CSV для обработки.
  * OUT_TRAIN: Путь к файлу для сохранения тренировочного набора данных.
  * OUT_VAL: Путь к файлу для сохранения валидационного набора данных.
  * TRAIN_SIZE: Размер тренировочного набора данных в пропорциях (по умолчанию 0.8, то есть 80% данных будут использоваться для тренировки). 
2) Настройка логирования
Логирование настраивается для записи отчетов о выполнении процесса в файл log/preprocess_data.log.
3) Функция main():
  * Чтение и объединение данных:
    - Считывается первый файл CSV для инициализации основного DataFrame.
    - Далее все файлы из списка args.input объединяются в один DataFrame.
  * Извлечение идентификаторов и фильтрация данных:
    - Извлекается url_id из ссылки в колонке url.
    - Создается новый DataFrame с нужными колонками (url_id, total_meters, price).
    - Фильтруются данные по стоимости квартир (исключаются квартиры стоимостью более 30 млн рублей).
  * Разделение на тренировочный и валидационный наборы:
    - Данные разделяются на тренировочный и валидационный наборы в пропорциях, заданных аргументом --split.
  * Сохранение данных и логирование:
    - Тренировочные и валидационные наборы данных сохраняются в соответствующие файлы CSV.
    - Пишется сообщение в лог о завершении процесса.

Чтобы провести предварительную обработку данных нужно:  
1) В файле [preprocess_data.py](../src/preprocess_data.py) задать параметры:
```
IN_FILES = ['data/raw/...',
            'data/raw/...',
            'data/raw/...']

OUT_TRAIN = 'data/proc/train.csv'
OUT_TEST = 'data/proc/test.csv'

TRAIN_SIZE = ...
PRICE_THRESHOLD = ...
```
2) Запустить файл 
```sh
python src/preprocess_data.py
```    

### 5. Обучение модели 

1) Задание параметров
  * TRAIN_DATA: Путь к CSV файлу с тренировочными данными.
  * VAL_DATA: Путь к CSV файлу с валидационными данными.
  * MODEL_SAVE_PATH: Путь для сохранения обученной модели.
2) Настройка логирования
Логирование настраивается для записи отчетов о выполнении процесса в файл log/train_model.log.
3) Функция main():
  * Загрузка данных:
    - Считываются тренировочные данные из файла TRAIN_DATA.
    - Считываются валидационные данные из файла VAL_DATA. 
  * Обучение модели:
    - Создается и обучается модель линейной регрессии на тренировочных данных.
    - Обученная модель сохраняется в файл, указанный пользователем или по умолчанию в MODEL_SAVE_PATH.
  * Оценка модели:
    - Вычисляется коэффициент детерминации  на тренировочных данных.
    - Вычисляются предсказанные значения на валидационных данных.
    - Рассчитывается средняя абсолютная ошибка (MAE).
    - Логируются метрики модели, включая уравнение линейной регрессии.

Описание модели линейной регрессии и входных параметров для предсказания
Обученная модель линейной регрессии предсказывает цену недвижимости на основе площади (total_meters). Уравнение модели принимает следующий вид:

цена = с*площадь + интерцепт

где:

c - коэффициент модели, рассчитанный в процессе обучения.

интерцепт - сдвиг (постоянное значение) модели.

**Лучшая модель** - catboost на следующих переменных: 'author_type', 'floor', 'floors_count', 'rooms_count', 'total_meters', 'underground' 

Для обучения модели необходимо:
1) В файле [train_model.py](../src/train_model.py) задать параметры:
```
TRAIN_DATA = 'data/proc/train.csv'
TEST_DATA = 'data/proc/test.csv'
MODEL_SAVE_PATH = 'models/catboost_v1.joblib'
```
2) Запустить файл 
```sh
python src/train_model.py
``` 
3) Для проверки работы модели запустить файл:
 ```sh
python test/test_api.py
``` 

### 6. Запуск приложения flask 

адресс сервера - http://176.123.163.78:8000.

Отчет по работе с dev- и prod-серверами находится [тут](docs/report_3.md)

### 7. Использование сервиса через веб интерфейс 

Для использования сервиса используйте файл `web/index.html`.  

### 8. Docker

```sh
# gunicorn
docker run -dp 8000:8000/tcp apoikalainen/pabd24:gunicorn
```
# Предиктивная аналитика больших данных

Учебный проект для демонстрации основных этапов жизненного цикла проекта предиктивной аналитики.  

## Installation 

Клонируйте репозиторий, создайте виртуальное окружение, активируйте и установите зависимости:  

```sh
git clone https://github.com/yourgit/pabd24
cd pabd24
python -m venv venv

source venv/bin/activate  # mac or linux
.\venv\Scripts\activate   # windows

pip install -r requirements.txt
```

## Usage

### 1. Сбор данных о ценах на недвижимость 

1) Создание экземпляра парсера:
Объект CianParser создается для конкретного города (в данном случае, Москва).
2) Основная функция main():
  * Определяется текущее время для создания уникального имени файла.
  * Указывается количество комнат в квартире (например, 3).
  * Определяется путь для сохранения данных в формате CSV.
  * Используя метод get_flats библиотеки cianparser, собираются данные о трехкомнатных квартирах на продажу.
  * Дополнительные параметры:
    - Тип сделки: продажа (deal_type="sale").
    - Количество комнат: 3.
    - Сохранение в CSV: автоматически сохраняет также в CSV.
    - Дополнительные настройки:
      - Начальная и конечная страницы для парсинга (1 и 2).
      - Тип объекта: вторичное жилье.
  * Полученные данные преобразуются в формат DataFrame с использованием библиотеки pandas.
  * Данные сохраняются в указанный CSV файл с указанной кодировкой (utf-8) и без индекса.

### 2. Выгрузка данных в хранилище S3 

1) Для доступа к хранилищу скопируйте файл `.env` в корень проекта.  
2)  Задание параметров:
  * BUCKET_NAME: имя целевого S3 бакета.
  * YOUR_ID: идентификатор пользователя или проекта.
  * CSV_PATH: список путей к локальным CSV-файлам, которые будут загружены в S3.
3) Загрузка конфигурации из файла .env:
Значения для KEY и SECRET загружаются из .env файла для обеспечения безопасности.
4) Функция main():
  * Создание клиента S3: Используются учетные данные из .env файла и указанный endpoint_url для подключения к S3 совместимому хранилищу (в данном случае Яндекс Облако).
  * Загрузка файлов:
    - Каждый файл из списка args.input загружается в указанное хранилище S3.
    - Имя объекта в бакете формируется на основе локального пути с добавлением идентификатора пользователя.
### 3. Загрузка данных из S3 на локальную машину  
1) Задание параметров: 
  * BUCKET_NAME: Имя бакета в S3, куда будут загружены файлы.
  * YOUR_ID: Идентификатор пользователя или проекта, используемый для создания уникальных имен файлов в бакете S3.
  * CSV_PATH: Список локальных файлов CSV для загрузки в S3 по умолчанию.
2) Загрузка конфигурации из файла .env:
Значения для KEY и SECRET загружаются из .env файла для обеспечения безопасности.
3) Основная функция main()
Функция main() выполняет выгрузку файлов в S3. Она принимает аргументы командной строки и для каждого файла из списка путей к файлам (args.input) осуществляет загрузку в указанный бакет S3.
  * remote_name: Имя файла в бакете S3 формируется путем добавления идентификатора пользователя к пути локального файла.
  * Метод client.upload_file загружает локальный файл по указанному пути в S3 бакет.

### 4. Предварительная обработка данных  

1) Задание параметров
  * IN_FILES: Список путей к исходным файлам CSV для обработки.
  * OUT_TRAIN: Путь к файлу для сохранения тренировочного набора данных.
  * OUT_VAL: Путь к файлу для сохранения валидационного набора данных.
  * TRAIN_SIZE: Размер тренировочного набора данных в пропорциях (по умолчанию 0.9, то есть 90% данных будут использоваться для тренировки). 
2) Настройка логирования
Логирование настраивается для записи отчетов о выполнении процесса в файл log/preprocess_data.log.
3) Функция main():
  * Чтение и объединение данных:
    - Считывается первый файл CSV для инициализации основного DataFrame.
    - Далее все файлы из списка args.input объединяются в один DataFrame.
  * Извлечение идентификаторов и фильтрация данных:
    - Извлекается url_id из ссылки в колонке url.
    - Создается новый DataFrame с нужными колонками (url_id, total_meters, price).
    - Фильтруются данные по стоимости квартир (исключаются квартиры стоимостью более 30 млн рублей).
  * Разделение на тренировочный и валидационный наборы:
    - Данные разделяются на тренировочный и валидационный наборы в пропорциях, заданных аргументом --split.
  * Сохранение данных и логирование:
    - Тренировочные и валидационные наборы данных сохраняются в соответствующие файлы CSV.
    - Пишется сообщение в лог о завершении процесса.

### 5. Обучение модели 

1) Задание параметров
  * TRAIN_DATA: Путь к CSV файлу с тренировочными данными.
  * VAL_DATA: Путь к CSV файлу с валидационными данными.
  * MODEL_SAVE_PATH: Путь для сохранения обученной модели.
2) Настройка логирования
Логирование настраивается для записи отчетов о выполнении процесса в файл log/train_model.log.
3) Функция main():
  * Загрузка данных:
    - Считываются тренировочные данные из файла TRAIN_DATA.
    - Считываются валидационные данные из файла VAL_DATA. 
  * Обучение модели:
    - Создается и обучается модель линейной регрессии на тренировочных данных.
    - Обученная модель сохраняется в файл, указанный пользователем или по умолчанию в MODEL_SAVE_PATH.
  * Оценка модели:
    - Вычисляется коэффициент детерминации  на тренировочных данных.
    - Вычисляются предсказанные значения на валидационных данных.
    - Рассчитывается средняя абсолютная ошибка (MAE).
    - Логируются метрики модели, включая уравнение линейной регрессии.

Описание модели и входных параметров для предсказания
Обученная модель линейной регрессии предсказывает цену недвижимости на основе площади (total_meters). Уравнение модели принимает следующий вид:
цена = с*площадь + интерцепт
где:
c - коэффициент модели, рассчитанный в процессе обучения.
интерцепт - сдвиг (постоянное значение) модели.

### 6. Запуск приложения flask 

адресс сервера - http://176.123.163.78:8000.

Отчет по работе с dev- и prod-серверами находится [тут](docs/report_3.md)

### 7. Использование сервиса через веб интерфейс 

Для использования сервиса используйте файл `web/index.html`.  

